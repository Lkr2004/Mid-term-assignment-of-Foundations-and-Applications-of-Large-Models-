import argparse
import os
import random
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import sentencepiece as spm
from pathlib import Path
from model import TransformerModel, generate_square_subsequent_mask
from data import load_or_download_iwslt, build_corpus_files, train_sentencepiece
import evaluate
from tqdm import tqdm
import sacrebleu
import numpy as np


# ---------------- å›ºå®šéšæœºç§å­ ----------------
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"ğŸ”’ éšæœºç§å­è®¾ç½®ä¸º: {seed}")


# ---------------- æ•°æ®é›†å°è£… ----------------
class TranslationDataset(Dataset):
    def __init__(self, hf_dataset, src_sp, tgt_sp, max_len=128):
        self.ds = hf_dataset
        self.src_sp = src_sp
        self.tgt_sp = tgt_sp
        self.max_len = max_len

    def __len__(self):
        return len(self.ds)

    def __getitem__(self, idx):
        item = self.ds[idx]['translation']
        src = item['en']
        tgt = item['de']
        src_ids = self.src_sp.encode(src, out_type=int)
        tgt_ids = self.tgt_sp.encode(tgt, out_type=int)
        src_ids = src_ids[:self.max_len-2]
        tgt_ids = tgt_ids[:self.max_len-2]
        src_ids = [2] + src_ids + [3]  # æ·»åŠ  BOS å’Œ EOS
        tgt_ids = [2] + tgt_ids + [3]
        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long), src, tgt


# ---------------- DataLoader æ‹¼æ¥ ----------------
def collate_fn(batch, pad_id=0):
    src_batch, tgt_batch, src_texts, tgt_texts = zip(*batch)
    max_src = max(len(x) for x in src_batch)
    max_tgt = max(len(x) for x in tgt_batch)
    src_padded = torch.zeros(len(batch), max_src, dtype=torch.long)
    tgt_padded = torch.zeros(len(batch), max_tgt, dtype=torch.long)
    for i, s in enumerate(src_batch):
        src_padded[i, :s.size(0)] = s
    for i, t in enumerate(tgt_batch):
        tgt_padded[i, :t.size(0)] = t
    return src_padded, tgt_padded, src_texts, tgt_texts


# ---------------- ç¿»è¯‘ç”Ÿæˆå‡½æ•° ----------------
@torch.no_grad()
def translate_sentence(model, src_text, src_sp, tgt_sp, device, max_len=128, pad_id=0):
    """ç¿»è¯‘å•ä¸ªå¥å­"""
    model.eval()
    
    # ç¼–ç æºæ–‡æœ¬
    src_ids = src_sp.encode(src_text, out_type=int)
    src_ids = [2] + src_ids[:max_len-2] + [3]  # æ·»åŠ  BOS å’Œ EOS
    src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)
    src_key_padding_mask = (src_tensor == pad_id)
    
    # ç¼–ç 
    memory = model.encode(src_tensor, src_key_padding_mask=src_key_padding_mask)
    
    # è‡ªå›å½’ç”Ÿæˆ
    ys = torch.tensor([[2]], dtype=torch.long, device=device)  # BOS
    
    for step in range(max_len - 1):
        tgt_mask = generate_square_subsequent_mask(ys.size(1)).to(device)
        tgt_key_padding_mask = (ys == pad_id)
        
        out = model.decode(ys, memory, tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)
        logits = model.generator(out[:, -1:])
        next_token = logits.argmax(dim=-1)
        
        ys = torch.cat([ys, next_token], dim=1)
        
        # é‡åˆ° EOS åœæ­¢
        if next_token.item() == 3:
            break
    
    # è§£ç ä¸ºæ–‡æœ¬
    pred_ids = ys[0].cpu().tolist()
    pred_ids = [id for id in pred_ids if id not in [0, 2, 3]]  # ç§»é™¤ç‰¹æ®Šæ ‡è®°
    
    if not pred_ids:
        pred_text = ""
    else:
        pred_text = tgt_sp.decode(pred_ids)
    
    return pred_text


# ---------------- æµ‹è¯•å‡½æ•° ----------------
def test_model(model, test_loader, src_sp, tgt_sp, device, num_samples=50, max_len=128):
    """æµ‹è¯•æ¨¡å‹å¹¶è¾“å‡ºç¿»è¯‘ç»“æœ"""
    model.eval()
    
    print(f"ğŸ§ª å¼€å§‹æµ‹è¯•ï¼ŒéšæœºæŠ½å– {num_samples} ä¸ªæ ·æœ¬...")
    print("=" * 100)
    
    # éšæœºé€‰æ‹©æ ·æœ¬
    all_indices = list(range(len(test_loader.dataset)))
    selected_indices = random.sample(all_indices, min(num_samples, len(all_indices)))
    
    all_hypotheses = []
    all_references = []
    all_sources = []
    
    for idx in tqdm(selected_indices, desc="æµ‹è¯•è¿›åº¦"):
        try:
            # è·å–æ ·æœ¬
            src_tensor, tgt_tensor, src_text, ref_text = test_loader.dataset[idx]
            src_tensor = src_tensor.unsqueeze(0).to(device)
            
            # ç”Ÿæˆç¿»è¯‘
            pred_text = translate_sentence(model, src_text, src_sp, tgt_sp, device, max_len)
            
            # æ”¶é›†ç»“æœ
            all_sources.append(src_text)
            all_references.append(ref_text)
            all_hypotheses.append(pred_text)
            
        except Exception as e:
            print(f"å¤„ç†æ ·æœ¬ {idx} æ—¶å‡ºé”™: {e}")
            continue
    
    # è¾“å‡ºéƒ¨åˆ†ç¿»è¯‘ç¤ºä¾‹
    print("\nğŸ“Š ç¿»è¯‘ç¤ºä¾‹:")
    print("=" * 100)
    for i in range(min(10, len(all_sources))):
        print(f"\næ ·æœ¬ {i+1}:")
        print(f"æºæ–‡ (EN): {all_sources[i]}")
        print(f"å‚è€ƒè¯‘æ–‡ (DE): {all_references[i]}")
        print(f"æ¨¡å‹è¯‘æ–‡ (DE): {all_hypotheses[i]}")
        print("-" * 80)
    
    # è®¡ç®— BLEU åˆ†æ•°
    if all_hypotheses:
        try:
            bleu = sacrebleu.corpus_bleu(all_hypotheses, [all_references])
            print(f"\nğŸ¯ æµ‹è¯•ç»“æœ (åŸºäº {len(all_hypotheses)} ä¸ªæ ·æœ¬):")
            print(f"BLEU åˆ†æ•°: {bleu.score:.2f}")
            print(f"è¯¦ç»†ç»Ÿè®¡:")
            print(f"  - ç²¾ç¡®åº¦: {bleu.precisions}")
            print(f"  - é•¿åº¦æ¯”ç‡: {bleu.ratio:.2f}")
            print(f"  - ç¿»è¯‘é•¿åº¦: {bleu.sys_len}, å‚è€ƒé•¿åº¦: {bleu.ref_len}")
        except Exception as e:
            print(f"è®¡ç®— BLEU æ—¶å‡ºé”™: {e}")
    else:
        print("âŒ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„ç¿»è¯‘ç»“æœ")
    
    return all_sources, all_references, all_hypotheses


# ---------------- ä¸»å‡½æ•° ----------------
def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•è®­ç»ƒå¥½çš„ç¿»è¯‘æ¨¡å‹")
    parser.add_argument('--model_path', type=str, default='/root/autodl-tmp/large-model/src/checkpoints/PE4/model_epoch10.pt', 
                       help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--src_sp_model', type=str, default='/root/autodl-tmp/large-model/src/sentence/spm_src.model',
                       help='æºè¯­è¨€åˆ†è¯å™¨è·¯å¾„')
    parser.add_argument('--tgt_sp_model', type=str, default='/root/autodl-tmp/large-model/src/sentence/spm_tgt.model',
                       help='ç›®æ ‡è¯­è¨€åˆ†è¯å™¨è·¯å¾„')
    parser.add_argument('--num_samples', type=int, default=50,
                       help='éšæœºæµ‹è¯•çš„æ ·æœ¬æ•°é‡')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--output_file', type=str, default='/root/autodl-tmp/large-model/results/test_results.csv',
                       help='ç»“æœä¿å­˜æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'ğŸ–¥ï¸  è®¾å¤‡: {device}')
    
    # åŠ è½½åˆ†è¯å™¨
    print("ğŸ“– åŠ è½½åˆ†è¯å™¨...")
    src_sp = spm.SentencePieceProcessor(model_file=args.src_sp_model)
    tgt_sp = spm.SentencePieceProcessor(model_file=args.tgt_sp_model)
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ¤– åŠ è½½æ¨¡å‹...")
    model = TransformerModel(
        len(src_sp), len(tgt_sp), 
        d_model=512, nhead=8,
        num_encoder_layers=4, num_decoder_layers=4,
        dim_feedforward=1024, dropout=0.1, max_len=128
    ).to(device)
    
    # åŠ è½½æ¨¡å‹æƒé‡
    model.load_state_dict(torch.load(args.model_path, map_location=device))
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {args.model_path}")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("ğŸ“š åŠ è½½æµ‹è¯•æ•°æ®...")
    ds = load_or_download_iwslt()
    test_data = ds['test']
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    test_dataset = TranslationDataset(test_data, src_sp, tgt_sp)
    test_loader = DataLoader(
        test_dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        collate_fn=collate_fn
    )
    
    print(f"ğŸ“Š æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
    
    # è¿›è¡Œæµ‹è¯•
    sources, references, hypotheses = test_model(
        model, test_loader, src_sp, tgt_sp, device, args.num_samples
    )
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼‰
    if args.output_file:
        with open(args.output_file, 'w', encoding='utf-8') as f:
            f.write("æºæ–‡,å‚è€ƒè¯‘æ–‡,æ¨¡å‹è¯‘æ–‡\n")
            for src, ref, hyp in zip(sources, references, hypotheses):
                f.write(f'"{src}","{ref}","{hyp}"\n')
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {args.output_file}")


if __name__ == '__main__':
    main()