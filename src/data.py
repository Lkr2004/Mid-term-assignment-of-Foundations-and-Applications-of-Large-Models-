import os
from datasets import load_dataset
import sentencepiece as spm
from pathlib import Path
import tempfile
import contextlib
import sys

def load_or_download_iwslt(local_arrow_dir='/root/autodl-tmp/large-model/data/iwslt2017-en-de', hf_name='iwslt2017', config='iwslt2017-en-de'):
    try:
        local_path = Path(local_arrow_dir)
        split_map = {}
        
        # éåŽ†ä¸‰ä¸ªå­æ–‡ä»¶å¤¹ï¼ˆtrain / validation / testï¼‰
        for split_name in ['train', 'validation', 'test']:
            split_dir = local_path / split_name
            if split_dir.exists():
                arrow_files = list(split_dir.glob('*.arrow'))
                if len(arrow_files) > 0:
                    split_map[split_name] = str(arrow_files[0])
        
        if len(split_map) > 0:
            print(f"âœ… Found local dataset splits: {list(split_map.keys())}")
            ds = load_dataset('arrow', data_files=split_map)
            return ds

        else:
            print("âš ï¸ No .arrow files found locally, will download dataset.")
    except Exception as e:
        print("âš ï¸ Local load failed, will download dataset instead:", e)

    # å¦‚æžœæœ¬åœ°åŠ è½½å¤±è´¥ï¼Œåˆ™ä»Ž Hugging Face ä¸‹è½½
    ds = load_dataset(hf_name, config)
    return ds

# è‡ªå®šä¹‰é™éŸ³ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šå±è”½C++å±‚stderrè¾“å‡º
@contextlib.contextmanager
def suppress_stdout_stderr():
    with open(os.devnull, 'w') as devnull:
        old_stdout = os.dup(1)
        old_stderr = os.dup(2)
        os.dup2(devnull.fileno(), 1)
        os.dup2(devnull.fileno(), 2)
        try:
            yield
        finally:
            os.dup2(old_stdout, 1)
            os.dup2(old_stderr, 2)
            os.close(old_stdout)
            os.close(old_stderr)

# åŸºäºŽç»™å®šçš„è¯­æ–™æ–‡ä»¶è®­ç»ƒSentencePieceæ¨¡åž‹
def train_sentencepiece(corpus_files, model_prefix='spm', vocab_size=8000, model_type='bpe', model_dir='/root/autodl-tmp/large-model/src/sentence'):
    if model_dir is None:
        model_dir = tempfile.mkdtemp()
    else:
        # ç¡®ä¿è‡ªå®šä¹‰ç›®å½•å­˜åœ¨
        os.makedirs(model_dir, exist_ok=True)

    cp = ','.join(corpus_files)
    model_prefix_path = os.path.join(model_dir, model_prefix)
    
    print(f"ðŸ§© Training SentencePiece model: {model_prefix_path}.model (vocab={vocab_size})")
    
    # âœ… ä½¿ç”¨ suppress_stdout_stderr() å±è”½æ‰€æœ‰åº•å±‚æ—¥å¿—
    with suppress_stdout_stderr():
        spm.SentencePieceTrainer.Train(
            input=cp,
            model_prefix=model_prefix_path,
            vocab_size=vocab_size,
            model_type=model_type,
            character_coverage=1.0,
            pad_id=0, unk_id=1, bos_id=2, eos_id=3
        )

    print(f"âœ… SentencePiece model saved to {model_prefix_path}.model")
    
    return model_prefix_path + '.model', model_prefix_path + '.vocab'

# ä»Ždatasetä¸­æŠ½å–æº/ç›®æ ‡å¥å­å¹¶å†™æˆä¸¤ä¸ªå¹³è¡Œæ–‡æœ¬æ–‡ä»¶
def build_corpus_files(dataset, src_lang='en', tgt_lang='de', out_dir='/root/autodl-tmp/large-model/src/tokenizer_corpus', max_samples=None):
    Path(out_dir).mkdir(exist_ok=True)
    src_path = os.path.join(out_dir, 'src.txt')
    tgt_path = os.path.join(out_dir, 'tgt.txt')
    n = 0
    with open(src_path, 'w', encoding='utf-8') as sf, open(tgt_path, 'w', encoding='utf-8') as tf:
        for ex in dataset:
            if max_samples and n >= max_samples:
                break
            src = ex['translation'][src_lang].strip()
            tgt = ex['translation'][tgt_lang].strip()
            if src and tgt:
                sf.write(src + '\n')
                tf.write(tgt + '\n')
                n += 1
    return src_path, tgt_path
